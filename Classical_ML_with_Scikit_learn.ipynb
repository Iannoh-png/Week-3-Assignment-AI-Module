{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3KdELawTQnibaEBkrqCJo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Iannoh-png/Week-3-Assignment-AI-Module/blob/main/Classical_ML_with_Scikit_learn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NbKm90nLQv3",
        "outputId": "e45f04fd-4d6a-477a-fd2a-29a742e1c798"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values per column:\n",
            " |               | 0   |\n",
            "|:--------------|:----|\n",
            "| Id            | 0   |\n",
            "| SepalLengthCm | 0   |\n",
            "| SepalWidthCm  | 0   |\n",
            "| PetalLengthCm | 0   |\n",
            "| PetalWidthCm  | 0   |\n",
            "| Species       | 0   |\n",
            "\n",
            "Encoded Species labels (first 5):\n",
            " [0 0 0 0 0]\n",
            "Original Species to Encoded Mapping:\n",
            " [('Iris-setosa', np.int64(0)), ('Iris-versicolor', np.int64(1)), ('Iris-virginica', np.int64(2))]\n",
            "\n",
            "Decision Tree Classifier trained successfully.\n",
            "\n",
            "Accuracy: 1.0000\n",
            "Precision: 1.0000\n",
            "Recall: 1.0000\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset (assuming 'Iris.csv' is in the same directory or path provided)\n",
        "df = pd.read_csv('Iris.csv')\n",
        "\n",
        "# --- 1. Preprocess the data ---\n",
        "\n",
        "# Check for missing values\n",
        "# Displays count of missing values for each column.\n",
        "print(\"Missing values per column:\\n\", df.isnull().sum().to_markdown(numalign=\"left\", stralign=\"left\"))\n",
        "\n",
        "# Drop the 'Id' column as it's not a feature for prediction\n",
        "# The 'Id' column is a unique identifier and does not contribute to the prediction.\n",
        "df_processed = df.drop('Id', axis=1)\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "# X contains the independent variables (features) used for prediction.\n",
        "X = df_processed.drop('Species', axis=1)\n",
        "# y contains the dependent variable (target) that we want to predict.\n",
        "y = df_processed['Species']\n",
        "\n",
        "# Encode the target variable 'Species' into numerical labels\n",
        "# Machine learning models require numerical input. LabelEncoder converts\n",
        "# categorical labels (like 'Iris-setosa') into numerical representations (like 0).\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "print(\"\\nEncoded Species labels (first 5):\\n\", y_encoded[:5])\n",
        "print(\"Original Species to Encoded Mapping:\\n\", list(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))\n",
        "\n",
        "\n",
        "# --- 2. Train a Decision Tree Classifier ---\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "# train_test_split divides the dataset into subsets for training and testing.\n",
        "# test_size=0.30 means 30% of the data will be used for testing, 70% for training.\n",
        "# random_state ensures reproducibility of the split; the same split is generated each time.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.30, random_state=42)\n",
        "\n",
        "# Initialize the Decision Tree Classifier\n",
        "# A Decision Tree is a non-parametric supervised learning method used for classification and regression.\n",
        "# random_state for reproducibility of the decision tree's internal randomness.\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier using the training data\n",
        "# The .fit() method trains the model using the features (X_train) and corresponding labels (y_train).\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nDecision Tree Classifier trained successfully.\")\n",
        "\n",
        "# --- 3. Evaluate using accuracy, precision, and recall ---\n",
        "\n",
        "# Make predictions on the test set\n",
        "# The trained model predicts the labels for the unseen test features (X_test).\n",
        "y_pred = dt_classifier.predict(X_test)\n",
        "\n",
        "# Calculate Accuracy\n",
        "# Accuracy is the proportion of correctly classified instances (true positives + true negatives)\n",
        "# out of the total instances.\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Calculate Precision\n",
        "# Precision is the ratio of true positives to the total predicted positives.\n",
        "# For multi-class classification, 'weighted' average is often used to account for class imbalance,\n",
        "# calculating metrics for each label and finding their average, weighted by the number of true instances for each label.\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "\n",
        "# Calculate Recall\n",
        "# Recall is the ratio of true positives to the total actual positives.\n",
        "# Similar to precision, 'weighted' average is used for multi-class problems.\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "print(f\"Recall: {recall:.4f}\")"
      ]
    }
  ]
}